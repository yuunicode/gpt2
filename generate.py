import torch
import pickle, json
from transformers import PreTrainedTokenizerFast
from model import GPT2Lite  # gpt2lite.py ë˜ëŠ” model.py ë‚´ì— ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•¨

# ê²½ë¡œ ì„¤ì •
MODEL_PATH = "./checkpoints/model-1000.pkl"  # ì›í•˜ëŠ” ì²´í¬í¬ì¸íŠ¸
TOKENIZER_PATH = "./tokenizer/sk_base_tokenizer.json"

# JSONì—ì„œ config ë¶ˆëŸ¬ì˜¤ê¸°
with open("config.json", "r") as f:
    config = json.load(f)

# í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = PreTrainedTokenizerFast(tokenizer_file=TOKENIZER_PATH)

# ëª¨ë¸ êµ¬ì„±
model = GPT2Lite(
    vocab_size=tokenizer.vocab_size,
    block_size=config["block_size"],
    n_layer=config["n_layer"],
    n_head=config["n_head"],
    n_embd=config["n_embd"]
)

# âœ… state_dict ë¡œë“œ (pkl ë°©ì‹)
with open(MODEL_PATH, "rb") as f:
    state_dict = pickle.load(f)
model.load_state_dict(state_dict)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval()

# âœ… ìƒì„± í•¨ìˆ˜
@torch.no_grad()
def generate_text(prompt, max_new_tokens=60):
    input_ids = tokenizer.encode(prompt, return_tensors="pt").to(device)
    output_ids = model.generate(input_ids, max_new_tokens=max_new_tokens)
    return tokenizer.decode(output_ids[0], skip_special_tokens=True)

# âœ… ì¸í„°ë™í‹°ë¸Œ ì‚¬ìš©
print("ğŸ§  GPT2Lite ëŒ€í™” ì‹œì‘ (ì¢…ë£Œ: 'exit')\n")
while True:
    prompt = input("ğŸ“ ì…ë ¥: ")
    if prompt.strip().lower() in ["exit", "quit"]:
        print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        break
    result = generate_text(prompt)
    print("ğŸ¤– GPT2Lite:", result)